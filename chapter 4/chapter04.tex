\documentclass[UTF8]{article}
\author {廖星宇}
\title {决策树}

\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}

\begin{document}
\maketitle
\section{基本流程}

决策树(decision tree)遵循“分而治之”(divide-and-conquer)策略，目的为了产生一棵泛化能力强的决策树。

三类情况会递归返回：

1. 当前节点所有样本属于同一类；

2.当前属性集为空，无法划分，标记为当前结点最多的类；

3.当前节点样本为空，标记为父节点最多的类
\\

\section{划分选择}

\subsection{信息增益}
“信息熵”(information entropy)定位为
\begin{equation}
Ent(D) = -\sum_{k=1}^{|y|} p_k log_2 p_k
\end{equation}
其中$p_k$表示样本集合中D中第k类样本所占的比例，Ent(D)的值越小，D的纯度越高。

“信息增益”(information gain)根据不同分支节点的样本数不同赋予不同的权重$|D^{v}|/|D|$，即样本数越多的分支结点的影响越大，定义为
\begin{equation}
  Gain(D, a) = Ent(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|}Ent(D^v)
\end{equation}

信息增益越大，说明使用属性a划分获得的“纯度提升”越大，所以选择属性可以通过下面的公式来得到
\begin{equation}
	a_{*} = \mathop{\arg\max}_{a \in A} \ \ Gain(D, a)
\end{equation}

\subsection{增益率}
信息增益对取值数目较多的属性有所偏好，为了减少这种偏好带来的影响，使用“增益率”(gain ratio)来选择最优划分属性。
\begin{equation}
  Gain \ ratio(D, a) = \frac{Gain(D, a)}{IV(a)}
\end{equation}

其中
\begin{equation}
  IV(a) = -\sum_{v=1}^{V}\frac{|D^v|}{|D|} log_2 \frac{|D^2|}{|D|}
\end{equation}

需要注意的是，增益率准则对可取数目较少的属性有所偏好。
\subsection{基尼指数}
CART决策树使用“基尼指数”(Gini index)来选择划分属性，数据集D的纯度可用基尼值来度量
\begin{equation}
  \begin{split}
  Gini(D) &= \sum_{k=1}^{|y|} \sum_{k^{\prime} \neq k} p_k p_{k^{\prime}}\\
  &= 1 - \sum_{k=1}^{|y|} p_{k}^{2}
  \end{split}
\end{equation}

Gini(D)反映了数据集中随机两个样本标定不一致的概率，所以Gini(D)越小，表示数据集D的纯度越高。

属性a的基尼指数定义为
\begin{equation}
  Gini\ index(D, a) = \sum_{v=1}^V \frac{|D^v|}{|D|} Gini(D^v)
\end{equation}

所以我们可以依据使得划分后基尼指数最小的属性作为最优划分属性，即
\begin{equation}
  a_{*} = \mathop{\arg\min}_{a \in A} Gini\ index(D, a)
\end{equation}

\section{剪枝处理}

剪纸(pruning)是决策树学习算法里面对付“过拟合”的主要手段，决策树为了尽可能正确分类训练样本，会将一些训练集自身的特点当做所有数据都具有的一般性质而导致过拟合，所以可以去掉一些分支来降低过拟合的风险。

常见的剪枝策略有“预剪枝”(prepruning)和“后剪枝”(postpruning)：

（1）预剪枝实在决策树生成过程中对每个节点在划分前进行估计；

（2）后剪枝是先训练一个完整的决策树，然后自底向上对非叶节点进行考察
\end{document}
